tag: ""
random_seed: 42

##############
# Dataloader #
##############
dataloader:
  # "arithmetic", "algebraic", "boolean" 
  dataset: "arithmetic"
  num_workers: 2

#########
# Model #
#########
model:
  # type: convmixer, mlpmixer, cnn, transformer
  type: "transformer"
  num_blocks: 2
  embedding_dim: 256   # For transformer: n_heads * head_dim!
  expansion_factor: 1  # only for MLP-Mixer
  hidden_expansion: 2
  kernel_size: 3       # only for ConvMixer / CNN

transformer:
  n_blocks: 4
  max_sequence_length: 64  # Let this be determined by dataset.
  token_embedding:
    # Available encoding schemes: random_normal
    encoding: "random_normal" 
  position_embedding:
    # Available encoding schemes: ones, zeros, random_normal, sinusoidal
    encoding: "zeros" 
  transformer_block:
    hidden_expansion: 2
    # expansion_factor: 2
    dropout_prob: 0
  self_attention:
    n_heads: 4
    head_dim: 64  # embedding dimension is n_heads * head_dim
    dropout_prob: 0
    use_bias: true
  mask:
    is_activated: true
    is_trainable: true
    # Available masks: causal, trainable_additive, trainable_multiplicative 
    type: "trainable_additive" 

###########
# Trainer #
###########
trainer:
  # gpu, cpu
  device: "gpu"
  num_update_steps: 10000000
  batch_size: 64
  learning_rate: 3.0e-4
  weight_decay: 0
  gradient_clipping:
    is_activated: false
    max_norm: 1.0

load_model:
  is_activated: false 
  model_name: "arithmetic"
  model_path: -1

########
# Data #
########
data:
  n_classes: Null
  input_shape: Null

###########
# Summary #
###########
summary:
  save_train_stats:
    every_n_updates: 100
  save_test_stats:
    every_n_updates: -1
  save_model:
    every_n_updates: 1000
  add_position_embeddings:
    every_n_updates: 1000
  add_token_embeddings:
    every_n_updates: 1000
  add_linear_weights:
    every_n_updates: 1000
  add_mask_weights:
    every_n_updates: 1000
  add_graph: false

###############
# Directories #
###############
dirs:
  data: "data"
  runs: "runs"
  weights: "weights"
